{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read before  \n",
    "- PCA/SVD \n",
    "- Try Except in python  \n",
    "- Latent semantic ANalysis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#necessary imports \n",
    "import nltk \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.decomposition import TruncatedSVD  \n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading Titles \n",
    "titles = [line.rstrip() for line in open('./data/latent_semantic_analysis/all_book_titles.txt')] \n",
    "stopwords = set(w.rstrip() for w in open('./data/stopwords.txt'))  \n",
    "stopwords = stopwords.union({'introduction','edition','series','application'\n",
    "                            'approach','card','access','package','plus','etext',\n",
    "                            'brief','vol','fundamental','guide','essential','printed',\n",
    "                            'third','second','fourth'}) #Adding more stop words specific to this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Reading all Titles,Tokenizing it and appending it to dictionary'''\n",
    "def my_tokenizer2(s): \n",
    "    s =s.lower()   #Lowercasing all the words\n",
    "    tokens = nltk.tokenize.word_tokenize(s)  #Tokenize the test\n",
    "    tokens = [t for t in tokens if len(t)>2] #only word len greater than 2 is useful \n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] \n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    tokens = [t for t in tokens if not any(c.isdigit() for c in t)]\n",
    "    return tokens  \n",
    "\n",
    "word_index_map = {} \n",
    "current_index = 0 \n",
    "all_tokens = [] \n",
    "all_titles = [] \n",
    "index_word_map = [] \n",
    "for title in titles: \n",
    "    try: \n",
    "        title = title.encode('ascii','ignore')\n",
    "        all_titles.append(title)\n",
    "        tokens = my_tokenizer2(title) \n",
    "        all_tokens.append(tokens) #Basically all Book titles tokenized.\n",
    "        for token in tokens:\n",
    "            if token not in word_index_map: \n",
    "                word_index_map[token] = current_index\n",
    "                current_index +=1\n",
    "                index_word_map.append(token)\n",
    "    except:\n",
    "        pass \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unsupervised learning (No label)\n",
    "def tokens_to_vector(tokens):\n",
    "    x = np.zeros(len(word_index_map)) \n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i]=1  \n",
    "    return x\n",
    "\n",
    "N = len(all_tokens) #All book titles[columns]\n",
    "D = len(word_index_map) #All possible words in all the titles[rows]\n",
    "X = np.zeros((D,N))\n",
    "i=0 \n",
    "for tokens in all_tokens:\n",
    "    X[:,i]=tokens_to_vector(tokens) #Here book titles are columns\n",
    "    i +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD()  \n",
    "Z = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(Z[:,0],Z[:,1])\n",
    "for i in xrange(D):\n",
    "    plt.annotate(s=index_word_map[i],xy=(Z[i,0],Z[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.axis([0,4,0,1])\n",
    "plt.savefig('books_read.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
